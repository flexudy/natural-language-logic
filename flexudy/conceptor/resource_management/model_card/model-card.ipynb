{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Towards Neuro-Symbolic Language Understanding\n",
    "\n",
    "![alt text](https://www.flexudy.com/wp-content/uploads/2021/09/conceptor.png \"Flexudy's conceptor\")\n",
    "\n",
    "At [Flexudy](https://flexudy.com), we look for ways to unify symbolic and sub-symbolic methods to improve model interpretation and inference.\n",
    "\n",
    "## Problem\n",
    "\n",
    "1. Word embeddings are awesome üöÄ. However, no one really knows what an array of 768 numbers means?\n",
    "2. Text/Token classification is also awesome ‚ù§Ô∏è‚Äç. Still, classifying things into a finite set of concepts is rather limited.\n",
    "3. Last but not least, how do I know that the word *cat* is a **mammal** and also an **animal** if my neural network is only trained to predict whether something is an animal or not?\n",
    "\n",
    "## Solution\n",
    "\n",
    "1. It would be cool if my neural network would just know that **cat** is an **animal** right? *‚àÄx.Cat(x) ‚áí Animal(x)*.\n",
    "Or for example, (*‚àÄx.Sch√∂neBlumen(x) ‚áí Blumen(x)*) -- English meaning: For all x, If x is a beautiful flower, then x is still a flower. --\n",
    "\n",
    "2. All of a sudden, tasks like **Question Answering**, **Summarization**, **Named Entity Recognition** or even **Intent Classification** etc become easier right?\n",
    "\n",
    "Well, one might probably still need time to build a good and robust solution that is not as large as **GPT3**.\n",
    "\n",
    "Like [Peter G√§rdenfors, author of conceptual spaces](https://www.goodreads.com/book/show/1877443.Conceptual_Spaces), we are trying to find ways to navigate between the symbolic and the sub-symbolic by thinking in concepts.\n",
    "\n",
    "Should such a solution exist, one could easily leverage true logical reasoning engines on natural language.\n",
    "\n",
    "How awesome would that be? üí°\n",
    "\n",
    "## Flexudy's Conceptor\n",
    "\n",
    "1. We developed a poor man's implementation of the ideal solution described above.\n",
    "2. Though it is a poor man's model, **it is still a useful one** ü§ó.\n",
    "\n",
    "### Usage\n",
    "\n",
    "No library should anyone suffer. Especially not if it is built on top of ü§ó **HF Transformers**.\n",
    "\n",
    "\n",
    "Go to the [Github repo](https://github.com/flexudy/natural-language-logic)\n",
    "\n",
    "\n",
    "```python\n",
    "from flexudy.conceptor.start import FlexudyConceptInferenceMachineFactory\n",
    "\n",
    "# Load me only once\n",
    "concept_inference_machine = FlexudyConceptInferenceMachineFactory.get_concept_inference_machine()\n",
    "\n",
    "# A list of terms.\n",
    "terms = [\"cat\", \"dog\", \"economics and sociology\", \"public company\"]\n",
    "\n",
    "# If you don't pass the language, a language detector will attempt to predict it for you\n",
    "# If any error occurs, the language defaults to English.\n",
    "language = \"en\"\n",
    "\n",
    "# Predict concepts\n",
    "# You can also pass the batch_size=2 and the beam_size=4\n",
    "concepts = concept_inference_machine.infer_concepts(terms, language=language)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```{<br/>'cat': ['mammal', 'animal'], <br/> 'dog': ['hound', 'animal'], <br/>'economics and sociology': ['both fields of study'], <br/>'public company': ['company']<br/>}```\n",
    "\n",
    "### How was it trained?\n",
    "\n",
    "1. Using Google's T5-base and T5-small. Both models are released on the Hugging Face Hub.\n",
    "2. T5-base was trained for only two epochs while T5-small was trained for 5 epochs.\n",
    "\n",
    "## Where did you get the data?\n",
    "\n",
    "1. I extracted and curated a fragment of [Conceptnet](https://conceptnet.io/)\n",
    "2. In particular, only the IsA relation was used.\n",
    "3. Note that one term can belong to multiple concepts (which is pretty cool if you think about [Fuzzy Description Logics](https://lat.inf.tu-dresden.de/~stefborg/Talks/QuantLAWorkshop2013.pdf)).\n",
    "Multiple inheritances however mean some terms belong to so many concepts. Hence, I decided to randomly throw away some due to the **maximum length limitation**.\n",
    "\n",
    "### Setup\n",
    "1. I finally allowed only `2` to `4` concepts at random for each term. This means, there is still great potential to make the models generalise better üöÄ.\n",
    "3. I used a total of `279884` training examples and `1260` for testing. Edges -- i.e `IsA(concept u, concept v)` -- in both sets are disjoint.\n",
    "4. Trained for `15K` steps with learning rate linear decay during each step. Starting at `0.001`\n",
    "5. Used `RAdam Optimiser` with weight_decay =`0.01` and batch_size =`36`.\n",
    "6. Source and target max length were both `64`.\n",
    "\n",
    "### Multilingual Models\n",
    "\n",
    "1. The \"conceptor\" model is multilingual. English, German and French is supported.\n",
    "2. [Conceptnet](https://conceptnet.io/) supports many languages, but I just chose those three because those are the ones I speak.\n",
    "\n",
    "### Metrics for flexudy-conceptor-t5-base\n",
    "\n",
    "| Metric        |         Score |\n",
    "| ------------- |:-------------:|\n",
    "| Exact Match   | 36.67         |\n",
    "| F1            | 43.08         |\n",
    "| Loss smooth   | 1.214         |\n",
    "\n",
    "Unfortunately, we no longer have the metrics for flexudy-conceptor-t5-small. If I recall correctly, base was just slightly better on the test set (ca. `2%` F1).\n",
    "\n",
    "## Why not just use the data if you have it structured already?\n",
    "\n",
    "Conceptnet is very large. Even if you just consider loading a fragment into your RAM, say with only 100K edges, this is still a large graph.\n",
    "Especially, if you think about how you will save the node embeddings efficiently for querying.\n",
    "If you prefer this approach, [Milvus](https://github.com/milvus-io/pymilvus) can be of great help.\n",
    "You can compute query embeddings and try to find the best match. From there (after matching), you can navigate through the graph at `100%` precision."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}